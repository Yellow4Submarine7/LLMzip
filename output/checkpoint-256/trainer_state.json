{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.1879071474447197,
  "eval_steps": 500,
  "global_step": 256,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0007340122947059363,
      "grad_norm": 0.781758725643158,
      "learning_rate": 2.0000000000000002e-07,
      "loss": 2.6462,
      "step": 1
    },
    {
      "epoch": 0.0014680245894118727,
      "grad_norm": 0.916190505027771,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 2.5178,
      "step": 2
    },
    {
      "epoch": 0.002202036884117809,
      "grad_norm": 0.7339825630187988,
      "learning_rate": 6.000000000000001e-07,
      "loss": 2.7203,
      "step": 3
    },
    {
      "epoch": 0.0029360491788237453,
      "grad_norm": 0.6893280148506165,
      "learning_rate": 8.000000000000001e-07,
      "loss": 2.5575,
      "step": 4
    },
    {
      "epoch": 0.0036700614735296817,
      "grad_norm": 0.884299099445343,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 2.5568,
      "step": 5
    },
    {
      "epoch": 0.004404073768235618,
      "grad_norm": 0.8215423226356506,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 2.6327,
      "step": 6
    },
    {
      "epoch": 0.005138086062941554,
      "grad_norm": 0.9456546306610107,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 2.6602,
      "step": 7
    },
    {
      "epoch": 0.005872098357647491,
      "grad_norm": 0.7900006771087646,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 2.5963,
      "step": 8
    },
    {
      "epoch": 0.006606110652353427,
      "grad_norm": 0.9240138530731201,
      "learning_rate": 1.8e-06,
      "loss": 2.4752,
      "step": 9
    },
    {
      "epoch": 0.007340122947059363,
      "grad_norm": 0.9961065649986267,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 2.6715,
      "step": 10
    },
    {
      "epoch": 0.0080741352417653,
      "grad_norm": 0.9289594292640686,
      "learning_rate": 2.2e-06,
      "loss": 2.4904,
      "step": 11
    },
    {
      "epoch": 0.008808147536471236,
      "grad_norm": 0.7675867080688477,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 2.5446,
      "step": 12
    },
    {
      "epoch": 0.009542159831177172,
      "grad_norm": 0.8438600897789001,
      "learning_rate": 2.6e-06,
      "loss": 2.4908,
      "step": 13
    },
    {
      "epoch": 0.010276172125883108,
      "grad_norm": 0.851679801940918,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 2.6305,
      "step": 14
    },
    {
      "epoch": 0.011010184420589045,
      "grad_norm": 0.9720341563224792,
      "learning_rate": 3e-06,
      "loss": 2.5647,
      "step": 15
    },
    {
      "epoch": 0.011744196715294981,
      "grad_norm": 0.9430860877037048,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 2.6545,
      "step": 16
    },
    {
      "epoch": 0.012478209010000917,
      "grad_norm": 0.7484952807426453,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 2.3967,
      "step": 17
    },
    {
      "epoch": 0.013212221304706853,
      "grad_norm": 1.025215744972229,
      "learning_rate": 3.6e-06,
      "loss": 2.6371,
      "step": 18
    },
    {
      "epoch": 0.01394623359941279,
      "grad_norm": 0.836677074432373,
      "learning_rate": 3.8e-06,
      "loss": 2.4195,
      "step": 19
    },
    {
      "epoch": 0.014680245894118727,
      "grad_norm": 0.7602584362030029,
      "learning_rate": 4.000000000000001e-06,
      "loss": 2.6723,
      "step": 20
    },
    {
      "epoch": 0.015414258188824663,
      "grad_norm": 0.9433746933937073,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 2.648,
      "step": 21
    },
    {
      "epoch": 0.0161482704835306,
      "grad_norm": 0.7135891914367676,
      "learning_rate": 4.4e-06,
      "loss": 2.602,
      "step": 22
    },
    {
      "epoch": 0.016882282778236536,
      "grad_norm": 0.8093212842941284,
      "learning_rate": 4.6e-06,
      "loss": 2.5216,
      "step": 23
    },
    {
      "epoch": 0.017616295072942472,
      "grad_norm": 0.9653185606002808,
      "learning_rate": 4.800000000000001e-06,
      "loss": 2.4027,
      "step": 24
    },
    {
      "epoch": 0.018350307367648408,
      "grad_norm": 1.032142162322998,
      "learning_rate": 5e-06,
      "loss": 2.5661,
      "step": 25
    },
    {
      "epoch": 0.019084319662354344,
      "grad_norm": 0.7932052612304688,
      "learning_rate": 5.2e-06,
      "loss": 2.5107,
      "step": 26
    },
    {
      "epoch": 0.01981833195706028,
      "grad_norm": 0.9256221055984497,
      "learning_rate": 5.4e-06,
      "loss": 2.6351,
      "step": 27
    },
    {
      "epoch": 0.020552344251766216,
      "grad_norm": 0.8536105155944824,
      "learning_rate": 5.600000000000001e-06,
      "loss": 2.8199,
      "step": 28
    },
    {
      "epoch": 0.021286356546472155,
      "grad_norm": 0.7565586566925049,
      "learning_rate": 5.8e-06,
      "loss": 2.3832,
      "step": 29
    },
    {
      "epoch": 0.02202036884117809,
      "grad_norm": 0.7845911383628845,
      "learning_rate": 6e-06,
      "loss": 2.6054,
      "step": 30
    },
    {
      "epoch": 0.022754381135884027,
      "grad_norm": 0.930156946182251,
      "learning_rate": 6.2e-06,
      "loss": 2.4518,
      "step": 31
    },
    {
      "epoch": 0.023488393430589963,
      "grad_norm": 0.9417481422424316,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 2.6209,
      "step": 32
    },
    {
      "epoch": 0.0242224057252959,
      "grad_norm": 0.9431471228599548,
      "learning_rate": 6.6e-06,
      "loss": 2.5744,
      "step": 33
    },
    {
      "epoch": 0.024956418020001835,
      "grad_norm": 0.8738349080085754,
      "learning_rate": 6.800000000000001e-06,
      "loss": 2.6032,
      "step": 34
    },
    {
      "epoch": 0.02569043031470777,
      "grad_norm": 0.9887378215789795,
      "learning_rate": 7.000000000000001e-06,
      "loss": 2.5003,
      "step": 35
    },
    {
      "epoch": 0.026424442609413706,
      "grad_norm": 0.8924866914749146,
      "learning_rate": 7.2e-06,
      "loss": 2.7113,
      "step": 36
    },
    {
      "epoch": 0.027158454904119642,
      "grad_norm": 0.9027833938598633,
      "learning_rate": 7.4e-06,
      "loss": 2.3375,
      "step": 37
    },
    {
      "epoch": 0.02789246719882558,
      "grad_norm": 0.940362811088562,
      "learning_rate": 7.6e-06,
      "loss": 2.7802,
      "step": 38
    },
    {
      "epoch": 0.028626479493531518,
      "grad_norm": 0.8430340886116028,
      "learning_rate": 7.8e-06,
      "loss": 2.709,
      "step": 39
    },
    {
      "epoch": 0.029360491788237453,
      "grad_norm": 1.1274144649505615,
      "learning_rate": 8.000000000000001e-06,
      "loss": 2.4926,
      "step": 40
    },
    {
      "epoch": 0.03009450408294339,
      "grad_norm": 0.8886633515357971,
      "learning_rate": 8.200000000000001e-06,
      "loss": 2.6319,
      "step": 41
    },
    {
      "epoch": 0.030828516377649325,
      "grad_norm": 0.9065758585929871,
      "learning_rate": 8.400000000000001e-06,
      "loss": 2.481,
      "step": 42
    },
    {
      "epoch": 0.03156252867235526,
      "grad_norm": 0.9111267924308777,
      "learning_rate": 8.599999999999999e-06,
      "loss": 2.6267,
      "step": 43
    },
    {
      "epoch": 0.0322965409670612,
      "grad_norm": 0.9120345711708069,
      "learning_rate": 8.8e-06,
      "loss": 2.7088,
      "step": 44
    },
    {
      "epoch": 0.03303055326176713,
      "grad_norm": 0.9072235822677612,
      "learning_rate": 9e-06,
      "loss": 2.4831,
      "step": 45
    },
    {
      "epoch": 0.03376456555647307,
      "grad_norm": 0.953757107257843,
      "learning_rate": 9.2e-06,
      "loss": 2.3967,
      "step": 46
    },
    {
      "epoch": 0.034498577851179005,
      "grad_norm": 0.8969557881355286,
      "learning_rate": 9.4e-06,
      "loss": 2.8063,
      "step": 47
    },
    {
      "epoch": 0.035232590145884944,
      "grad_norm": 0.9925126433372498,
      "learning_rate": 9.600000000000001e-06,
      "loss": 2.5315,
      "step": 48
    },
    {
      "epoch": 0.03596660244059088,
      "grad_norm": 0.9926469922065735,
      "learning_rate": 9.800000000000001e-06,
      "loss": 2.5263,
      "step": 49
    },
    {
      "epoch": 0.036700614735296816,
      "grad_norm": 1.03434419631958,
      "learning_rate": 1e-05,
      "loss": 2.5837,
      "step": 50
    },
    {
      "epoch": 0.037434627030002755,
      "grad_norm": 0.9961101412773132,
      "learning_rate": 1.02e-05,
      "loss": 2.5691,
      "step": 51
    },
    {
      "epoch": 0.03816863932470869,
      "grad_norm": 0.9641546607017517,
      "learning_rate": 1.04e-05,
      "loss": 2.4552,
      "step": 52
    },
    {
      "epoch": 0.03890265161941463,
      "grad_norm": 0.9756762385368347,
      "learning_rate": 1.06e-05,
      "loss": 2.3986,
      "step": 53
    },
    {
      "epoch": 0.03963666391412056,
      "grad_norm": 1.2420450448989868,
      "learning_rate": 1.08e-05,
      "loss": 2.5766,
      "step": 54
    },
    {
      "epoch": 0.0403706762088265,
      "grad_norm": 1.2943239212036133,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 2.6929,
      "step": 55
    },
    {
      "epoch": 0.04110468850353243,
      "grad_norm": 0.9517909288406372,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 2.5599,
      "step": 56
    },
    {
      "epoch": 0.04183870079823837,
      "grad_norm": 1.3145428895950317,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 2.4791,
      "step": 57
    },
    {
      "epoch": 0.04257271309294431,
      "grad_norm": 1.1191105842590332,
      "learning_rate": 1.16e-05,
      "loss": 2.3806,
      "step": 58
    },
    {
      "epoch": 0.04330672538765024,
      "grad_norm": 1.0665556192398071,
      "learning_rate": 1.18e-05,
      "loss": 2.5422,
      "step": 59
    },
    {
      "epoch": 0.04404073768235618,
      "grad_norm": 1.1969786882400513,
      "learning_rate": 1.2e-05,
      "loss": 2.5165,
      "step": 60
    },
    {
      "epoch": 0.044774749977062114,
      "grad_norm": 1.1164778470993042,
      "learning_rate": 1.22e-05,
      "loss": 2.5158,
      "step": 61
    },
    {
      "epoch": 0.045508762271768054,
      "grad_norm": 1.0340051651000977,
      "learning_rate": 1.24e-05,
      "loss": 2.6857,
      "step": 62
    },
    {
      "epoch": 0.046242774566473986,
      "grad_norm": 0.9334219098091125,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 2.6343,
      "step": 63
    },
    {
      "epoch": 0.046976786861179926,
      "grad_norm": 1.0307633876800537,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 2.597,
      "step": 64
    },
    {
      "epoch": 0.04771079915588586,
      "grad_norm": 1.2045567035675049,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 2.566,
      "step": 65
    },
    {
      "epoch": 0.0484448114505918,
      "grad_norm": 1.083350658416748,
      "learning_rate": 1.32e-05,
      "loss": 2.7318,
      "step": 66
    },
    {
      "epoch": 0.04917882374529774,
      "grad_norm": 0.9957767128944397,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 2.5554,
      "step": 67
    },
    {
      "epoch": 0.04991283604000367,
      "grad_norm": 1.206835150718689,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 2.6,
      "step": 68
    },
    {
      "epoch": 0.05064684833470961,
      "grad_norm": 1.0216346979141235,
      "learning_rate": 1.3800000000000002e-05,
      "loss": 2.7044,
      "step": 69
    },
    {
      "epoch": 0.05138086062941554,
      "grad_norm": 1.1833897829055786,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 2.4994,
      "step": 70
    },
    {
      "epoch": 0.05211487292412148,
      "grad_norm": 1.1484802961349487,
      "learning_rate": 1.42e-05,
      "loss": 2.5225,
      "step": 71
    },
    {
      "epoch": 0.05284888521882741,
      "grad_norm": 1.2040046453475952,
      "learning_rate": 1.44e-05,
      "loss": 2.5256,
      "step": 72
    },
    {
      "epoch": 0.05358289751353335,
      "grad_norm": 1.304303765296936,
      "learning_rate": 1.4599999999999999e-05,
      "loss": 2.3898,
      "step": 73
    },
    {
      "epoch": 0.054316909808239285,
      "grad_norm": 1.0734000205993652,
      "learning_rate": 1.48e-05,
      "loss": 2.6265,
      "step": 74
    },
    {
      "epoch": 0.055050922102945224,
      "grad_norm": 1.135278582572937,
      "learning_rate": 1.5e-05,
      "loss": 2.4962,
      "step": 75
    },
    {
      "epoch": 0.05578493439765116,
      "grad_norm": 1.1880226135253906,
      "learning_rate": 1.52e-05,
      "loss": 2.5151,
      "step": 76
    },
    {
      "epoch": 0.056518946692357096,
      "grad_norm": 1.3100135326385498,
      "learning_rate": 1.54e-05,
      "loss": 2.4661,
      "step": 77
    },
    {
      "epoch": 0.057252958987063035,
      "grad_norm": 1.430815577507019,
      "learning_rate": 1.56e-05,
      "loss": 2.591,
      "step": 78
    },
    {
      "epoch": 0.05798697128176897,
      "grad_norm": 1.2172799110412598,
      "learning_rate": 1.58e-05,
      "loss": 2.4814,
      "step": 79
    },
    {
      "epoch": 0.05872098357647491,
      "grad_norm": 1.2113826274871826,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 2.5198,
      "step": 80
    },
    {
      "epoch": 0.05945499587118084,
      "grad_norm": 1.1763405799865723,
      "learning_rate": 1.62e-05,
      "loss": 2.5911,
      "step": 81
    },
    {
      "epoch": 0.06018900816588678,
      "grad_norm": 1.210614562034607,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 2.6094,
      "step": 82
    },
    {
      "epoch": 0.06092302046059272,
      "grad_norm": 1.300028681755066,
      "learning_rate": 1.66e-05,
      "loss": 2.3426,
      "step": 83
    },
    {
      "epoch": 0.06165703275529865,
      "grad_norm": 1.1790786981582642,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 2.4274,
      "step": 84
    },
    {
      "epoch": 0.06239104505000459,
      "grad_norm": 1.346990704536438,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 2.478,
      "step": 85
    },
    {
      "epoch": 0.06312505734471052,
      "grad_norm": 1.45244300365448,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 2.421,
      "step": 86
    },
    {
      "epoch": 0.06385906963941645,
      "grad_norm": 1.2576768398284912,
      "learning_rate": 1.74e-05,
      "loss": 2.4654,
      "step": 87
    },
    {
      "epoch": 0.0645930819341224,
      "grad_norm": 1.5955497026443481,
      "learning_rate": 1.76e-05,
      "loss": 2.5012,
      "step": 88
    },
    {
      "epoch": 0.06532709422882833,
      "grad_norm": 1.2203136682510376,
      "learning_rate": 1.78e-05,
      "loss": 2.4486,
      "step": 89
    },
    {
      "epoch": 0.06606110652353427,
      "grad_norm": 1.515824317932129,
      "learning_rate": 1.8e-05,
      "loss": 2.3153,
      "step": 90
    },
    {
      "epoch": 0.06679511881824021,
      "grad_norm": 1.3785465955734253,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 2.4978,
      "step": 91
    },
    {
      "epoch": 0.06752913111294614,
      "grad_norm": 1.3090336322784424,
      "learning_rate": 1.84e-05,
      "loss": 2.4999,
      "step": 92
    },
    {
      "epoch": 0.06826314340765208,
      "grad_norm": 1.375572919845581,
      "learning_rate": 1.86e-05,
      "loss": 2.388,
      "step": 93
    },
    {
      "epoch": 0.06899715570235801,
      "grad_norm": 1.389599323272705,
      "learning_rate": 1.88e-05,
      "loss": 2.4344,
      "step": 94
    },
    {
      "epoch": 0.06973116799706396,
      "grad_norm": 1.5101710557937622,
      "learning_rate": 1.9e-05,
      "loss": 2.3498,
      "step": 95
    },
    {
      "epoch": 0.07046518029176989,
      "grad_norm": 1.4341682195663452,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 2.5377,
      "step": 96
    },
    {
      "epoch": 0.07119919258647582,
      "grad_norm": 1.4636965990066528,
      "learning_rate": 1.94e-05,
      "loss": 2.5936,
      "step": 97
    },
    {
      "epoch": 0.07193320488118175,
      "grad_norm": 1.2901415824890137,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 2.3947,
      "step": 98
    },
    {
      "epoch": 0.0726672171758877,
      "grad_norm": 1.3718156814575195,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 2.4068,
      "step": 99
    },
    {
      "epoch": 0.07340122947059363,
      "grad_norm": 1.515077829360962,
      "learning_rate": 2e-05,
      "loss": 2.5185,
      "step": 100
    },
    {
      "epoch": 0.07413524176529956,
      "grad_norm": 1.6466740369796753,
      "learning_rate": 2.0200000000000003e-05,
      "loss": 2.4478,
      "step": 101
    },
    {
      "epoch": 0.07486925406000551,
      "grad_norm": 1.4270682334899902,
      "learning_rate": 2.04e-05,
      "loss": 2.3179,
      "step": 102
    },
    {
      "epoch": 0.07560326635471144,
      "grad_norm": 1.5459810495376587,
      "learning_rate": 2.06e-05,
      "loss": 2.2028,
      "step": 103
    },
    {
      "epoch": 0.07633727864941738,
      "grad_norm": 1.3839410543441772,
      "learning_rate": 2.08e-05,
      "loss": 2.4143,
      "step": 104
    },
    {
      "epoch": 0.07707129094412331,
      "grad_norm": 1.5842556953430176,
      "learning_rate": 2.1e-05,
      "loss": 2.4321,
      "step": 105
    },
    {
      "epoch": 0.07780530323882925,
      "grad_norm": 1.4404544830322266,
      "learning_rate": 2.12e-05,
      "loss": 2.235,
      "step": 106
    },
    {
      "epoch": 0.07853931553353519,
      "grad_norm": 1.2121896743774414,
      "learning_rate": 2.1400000000000002e-05,
      "loss": 2.5355,
      "step": 107
    },
    {
      "epoch": 0.07927332782824112,
      "grad_norm": 1.442963719367981,
      "learning_rate": 2.16e-05,
      "loss": 2.3769,
      "step": 108
    },
    {
      "epoch": 0.08000734012294707,
      "grad_norm": 1.3415473699569702,
      "learning_rate": 2.18e-05,
      "loss": 2.4134,
      "step": 109
    },
    {
      "epoch": 0.080741352417653,
      "grad_norm": 1.5933077335357666,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 2.4622,
      "step": 110
    },
    {
      "epoch": 0.08147536471235893,
      "grad_norm": 1.474933385848999,
      "learning_rate": 2.22e-05,
      "loss": 2.0817,
      "step": 111
    },
    {
      "epoch": 0.08220937700706486,
      "grad_norm": 1.725272297859192,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 2.4835,
      "step": 112
    },
    {
      "epoch": 0.08294338930177081,
      "grad_norm": 2.1825201511383057,
      "learning_rate": 2.26e-05,
      "loss": 2.7667,
      "step": 113
    },
    {
      "epoch": 0.08367740159647674,
      "grad_norm": 1.4479985237121582,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 2.6816,
      "step": 114
    },
    {
      "epoch": 0.08441141389118267,
      "grad_norm": 1.5765353441238403,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 2.3861,
      "step": 115
    },
    {
      "epoch": 0.08514542618588862,
      "grad_norm": 1.6123203039169312,
      "learning_rate": 2.32e-05,
      "loss": 2.573,
      "step": 116
    },
    {
      "epoch": 0.08587943848059455,
      "grad_norm": 1.9016907215118408,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 2.2324,
      "step": 117
    },
    {
      "epoch": 0.08661345077530049,
      "grad_norm": 1.5670136213302612,
      "learning_rate": 2.36e-05,
      "loss": 2.491,
      "step": 118
    },
    {
      "epoch": 0.08734746307000642,
      "grad_norm": 1.7287105321884155,
      "learning_rate": 2.38e-05,
      "loss": 2.3869,
      "step": 119
    },
    {
      "epoch": 0.08808147536471236,
      "grad_norm": 1.469605565071106,
      "learning_rate": 2.4e-05,
      "loss": 2.385,
      "step": 120
    },
    {
      "epoch": 0.0888154876594183,
      "grad_norm": 1.4345871210098267,
      "learning_rate": 2.4200000000000002e-05,
      "loss": 2.4418,
      "step": 121
    },
    {
      "epoch": 0.08954949995412423,
      "grad_norm": 1.594031572341919,
      "learning_rate": 2.44e-05,
      "loss": 2.4053,
      "step": 122
    },
    {
      "epoch": 0.09028351224883016,
      "grad_norm": 1.6396867036819458,
      "learning_rate": 2.46e-05,
      "loss": 2.3252,
      "step": 123
    },
    {
      "epoch": 0.09101752454353611,
      "grad_norm": 1.5790328979492188,
      "learning_rate": 2.48e-05,
      "loss": 2.4761,
      "step": 124
    },
    {
      "epoch": 0.09175153683824204,
      "grad_norm": 1.5084565877914429,
      "learning_rate": 2.5e-05,
      "loss": 2.4447,
      "step": 125
    },
    {
      "epoch": 0.09248554913294797,
      "grad_norm": 2.0525712966918945,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 2.2611,
      "step": 126
    },
    {
      "epoch": 0.09321956142765392,
      "grad_norm": 1.6794803142547607,
      "learning_rate": 2.54e-05,
      "loss": 2.2956,
      "step": 127
    },
    {
      "epoch": 0.09395357372235985,
      "grad_norm": 2.349001169204712,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 2.3631,
      "step": 128
    },
    {
      "epoch": 0.09468758601706578,
      "grad_norm": 1.7755742073059082,
      "learning_rate": 2.58e-05,
      "loss": 2.3633,
      "step": 129
    },
    {
      "epoch": 0.09542159831177172,
      "grad_norm": 1.5781538486480713,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 2.4585,
      "step": 130
    },
    {
      "epoch": 0.09615561060647766,
      "grad_norm": 1.8018994331359863,
      "learning_rate": 2.6200000000000003e-05,
      "loss": 2.3635,
      "step": 131
    },
    {
      "epoch": 0.0968896229011836,
      "grad_norm": 1.7671997547149658,
      "learning_rate": 2.64e-05,
      "loss": 2.475,
      "step": 132
    },
    {
      "epoch": 0.09762363519588953,
      "grad_norm": 1.7310080528259277,
      "learning_rate": 2.6600000000000003e-05,
      "loss": 2.2827,
      "step": 133
    },
    {
      "epoch": 0.09835764749059547,
      "grad_norm": 2.021481513977051,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 2.388,
      "step": 134
    },
    {
      "epoch": 0.0990916597853014,
      "grad_norm": 1.9279682636260986,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 2.4437,
      "step": 135
    },
    {
      "epoch": 0.09982567208000734,
      "grad_norm": 1.5232689380645752,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 2.3308,
      "step": 136
    },
    {
      "epoch": 0.10055968437471327,
      "grad_norm": 1.7168899774551392,
      "learning_rate": 2.7400000000000002e-05,
      "loss": 2.5057,
      "step": 137
    },
    {
      "epoch": 0.10129369666941922,
      "grad_norm": 1.8720086812973022,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 2.2924,
      "step": 138
    },
    {
      "epoch": 0.10202770896412515,
      "grad_norm": 1.554263710975647,
      "learning_rate": 2.7800000000000005e-05,
      "loss": 2.5207,
      "step": 139
    },
    {
      "epoch": 0.10276172125883108,
      "grad_norm": 1.4751852750778198,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 2.473,
      "step": 140
    },
    {
      "epoch": 0.10349573355353703,
      "grad_norm": 1.655071496963501,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 2.4927,
      "step": 141
    },
    {
      "epoch": 0.10422974584824296,
      "grad_norm": 1.71048104763031,
      "learning_rate": 2.84e-05,
      "loss": 2.2589,
      "step": 142
    },
    {
      "epoch": 0.10496375814294889,
      "grad_norm": 1.7448079586029053,
      "learning_rate": 2.86e-05,
      "loss": 2.362,
      "step": 143
    },
    {
      "epoch": 0.10569777043765483,
      "grad_norm": 1.7001092433929443,
      "learning_rate": 2.88e-05,
      "loss": 2.292,
      "step": 144
    },
    {
      "epoch": 0.10643178273236077,
      "grad_norm": 1.7244017124176025,
      "learning_rate": 2.9e-05,
      "loss": 2.3692,
      "step": 145
    },
    {
      "epoch": 0.1071657950270667,
      "grad_norm": 1.8922427892684937,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 2.3457,
      "step": 146
    },
    {
      "epoch": 0.10789980732177264,
      "grad_norm": 2.2304537296295166,
      "learning_rate": 2.94e-05,
      "loss": 2.3289,
      "step": 147
    },
    {
      "epoch": 0.10863381961647857,
      "grad_norm": 5.015443801879883,
      "learning_rate": 2.96e-05,
      "loss": 2.4082,
      "step": 148
    },
    {
      "epoch": 0.10936783191118452,
      "grad_norm": 1.4805980920791626,
      "learning_rate": 2.98e-05,
      "loss": 2.3723,
      "step": 149
    },
    {
      "epoch": 0.11010184420589045,
      "grad_norm": 1.8601855039596558,
      "learning_rate": 3e-05,
      "loss": 2.624,
      "step": 150
    },
    {
      "epoch": 0.11083585650059638,
      "grad_norm": 1.7565606832504272,
      "learning_rate": 3.02e-05,
      "loss": 2.4732,
      "step": 151
    },
    {
      "epoch": 0.11156986879530233,
      "grad_norm": 1.7545034885406494,
      "learning_rate": 3.04e-05,
      "loss": 2.3769,
      "step": 152
    },
    {
      "epoch": 0.11230388109000826,
      "grad_norm": 2.15533709526062,
      "learning_rate": 3.06e-05,
      "loss": 2.4138,
      "step": 153
    },
    {
      "epoch": 0.11303789338471419,
      "grad_norm": 2.047950506210327,
      "learning_rate": 3.08e-05,
      "loss": 2.3395,
      "step": 154
    },
    {
      "epoch": 0.11377190567942012,
      "grad_norm": 1.684034824371338,
      "learning_rate": 3.1e-05,
      "loss": 2.3467,
      "step": 155
    },
    {
      "epoch": 0.11450591797412607,
      "grad_norm": 1.8774583339691162,
      "learning_rate": 3.12e-05,
      "loss": 2.3966,
      "step": 156
    },
    {
      "epoch": 0.115239930268832,
      "grad_norm": 1.6214014291763306,
      "learning_rate": 3.1400000000000004e-05,
      "loss": 2.5243,
      "step": 157
    },
    {
      "epoch": 0.11597394256353794,
      "grad_norm": 1.819440245628357,
      "learning_rate": 3.16e-05,
      "loss": 2.4495,
      "step": 158
    },
    {
      "epoch": 0.11670795485824388,
      "grad_norm": 2.051161527633667,
      "learning_rate": 3.18e-05,
      "loss": 2.1429,
      "step": 159
    },
    {
      "epoch": 0.11744196715294981,
      "grad_norm": 1.8200809955596924,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 2.5714,
      "step": 160
    },
    {
      "epoch": 0.11817597944765575,
      "grad_norm": 2.6285715103149414,
      "learning_rate": 3.2200000000000003e-05,
      "loss": 2.2804,
      "step": 161
    },
    {
      "epoch": 0.11890999174236168,
      "grad_norm": 1.699316143989563,
      "learning_rate": 3.24e-05,
      "loss": 2.3,
      "step": 162
    },
    {
      "epoch": 0.11964400403706762,
      "grad_norm": 1.8041613101959229,
      "learning_rate": 3.26e-05,
      "loss": 2.5387,
      "step": 163
    },
    {
      "epoch": 0.12037801633177356,
      "grad_norm": 1.9045177698135376,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 2.3537,
      "step": 164
    },
    {
      "epoch": 0.12111202862647949,
      "grad_norm": 1.8874424695968628,
      "learning_rate": 3.3e-05,
      "loss": 2.2075,
      "step": 165
    },
    {
      "epoch": 0.12184604092118544,
      "grad_norm": 2.7796261310577393,
      "learning_rate": 3.32e-05,
      "loss": 2.3696,
      "step": 166
    },
    {
      "epoch": 0.12258005321589137,
      "grad_norm": 1.9007372856140137,
      "learning_rate": 3.3400000000000005e-05,
      "loss": 2.3295,
      "step": 167
    },
    {
      "epoch": 0.1233140655105973,
      "grad_norm": 1.9391717910766602,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 2.2333,
      "step": 168
    },
    {
      "epoch": 0.12404807780530323,
      "grad_norm": 1.9926397800445557,
      "learning_rate": 3.38e-05,
      "loss": 2.453,
      "step": 169
    },
    {
      "epoch": 0.12478209010000918,
      "grad_norm": 2.2730538845062256,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 2.4098,
      "step": 170
    },
    {
      "epoch": 0.1255161023947151,
      "grad_norm": 1.9852588176727295,
      "learning_rate": 3.4200000000000005e-05,
      "loss": 2.2339,
      "step": 171
    },
    {
      "epoch": 0.12625011468942104,
      "grad_norm": 1.9688912630081177,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 2.2263,
      "step": 172
    },
    {
      "epoch": 0.12698412698412698,
      "grad_norm": 1.8701858520507812,
      "learning_rate": 3.46e-05,
      "loss": 2.3529,
      "step": 173
    },
    {
      "epoch": 0.1277181392788329,
      "grad_norm": 2.0292179584503174,
      "learning_rate": 3.48e-05,
      "loss": 2.4222,
      "step": 174
    },
    {
      "epoch": 0.12845215157353887,
      "grad_norm": 2.1715011596679688,
      "learning_rate": 3.5e-05,
      "loss": 2.4745,
      "step": 175
    },
    {
      "epoch": 0.1291861638682448,
      "grad_norm": 1.6623706817626953,
      "learning_rate": 3.52e-05,
      "loss": 2.3305,
      "step": 176
    },
    {
      "epoch": 0.12992017616295073,
      "grad_norm": 2.0386931896209717,
      "learning_rate": 3.54e-05,
      "loss": 2.1703,
      "step": 177
    },
    {
      "epoch": 0.13065418845765667,
      "grad_norm": 2.5569655895233154,
      "learning_rate": 3.56e-05,
      "loss": 2.1578,
      "step": 178
    },
    {
      "epoch": 0.1313882007523626,
      "grad_norm": 1.991744875907898,
      "learning_rate": 3.58e-05,
      "loss": 2.3346,
      "step": 179
    },
    {
      "epoch": 0.13212221304706853,
      "grad_norm": 2.441236972808838,
      "learning_rate": 3.6e-05,
      "loss": 2.204,
      "step": 180
    },
    {
      "epoch": 0.13285622534177446,
      "grad_norm": 1.9558912515640259,
      "learning_rate": 3.62e-05,
      "loss": 2.1215,
      "step": 181
    },
    {
      "epoch": 0.13359023763648042,
      "grad_norm": 2.0833215713500977,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 2.3738,
      "step": 182
    },
    {
      "epoch": 0.13432424993118636,
      "grad_norm": 2.086308479309082,
      "learning_rate": 3.66e-05,
      "loss": 2.3512,
      "step": 183
    },
    {
      "epoch": 0.1350582622258923,
      "grad_norm": 1.7879992723464966,
      "learning_rate": 3.68e-05,
      "loss": 2.243,
      "step": 184
    },
    {
      "epoch": 0.13579227452059822,
      "grad_norm": 2.301527738571167,
      "learning_rate": 3.7e-05,
      "loss": 2.3765,
      "step": 185
    },
    {
      "epoch": 0.13652628681530415,
      "grad_norm": 1.9541267156600952,
      "learning_rate": 3.72e-05,
      "loss": 2.3074,
      "step": 186
    },
    {
      "epoch": 0.1372602991100101,
      "grad_norm": 2.5821235179901123,
      "learning_rate": 3.74e-05,
      "loss": 2.295,
      "step": 187
    },
    {
      "epoch": 0.13799431140471602,
      "grad_norm": 2.3519601821899414,
      "learning_rate": 3.76e-05,
      "loss": 2.5003,
      "step": 188
    },
    {
      "epoch": 0.13872832369942195,
      "grad_norm": 2.379178047180176,
      "learning_rate": 3.7800000000000004e-05,
      "loss": 2.4201,
      "step": 189
    },
    {
      "epoch": 0.1394623359941279,
      "grad_norm": 1.961816430091858,
      "learning_rate": 3.8e-05,
      "loss": 2.325,
      "step": 190
    },
    {
      "epoch": 0.14019634828883384,
      "grad_norm": 2.387526273727417,
      "learning_rate": 3.82e-05,
      "loss": 2.4353,
      "step": 191
    },
    {
      "epoch": 0.14093036058353978,
      "grad_norm": 2.249311685562134,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 2.2284,
      "step": 192
    },
    {
      "epoch": 0.1416643728782457,
      "grad_norm": 2.385368585586548,
      "learning_rate": 3.86e-05,
      "loss": 2.2445,
      "step": 193
    },
    {
      "epoch": 0.14239838517295164,
      "grad_norm": 2.1651339530944824,
      "learning_rate": 3.88e-05,
      "loss": 2.201,
      "step": 194
    },
    {
      "epoch": 0.14313239746765757,
      "grad_norm": 2.2181589603424072,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 2.1889,
      "step": 195
    },
    {
      "epoch": 0.1438664097623635,
      "grad_norm": 2.1362600326538086,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 2.2888,
      "step": 196
    },
    {
      "epoch": 0.14460042205706947,
      "grad_norm": 2.311180591583252,
      "learning_rate": 3.94e-05,
      "loss": 2.4389,
      "step": 197
    },
    {
      "epoch": 0.1453344343517754,
      "grad_norm": 2.0274317264556885,
      "learning_rate": 3.960000000000001e-05,
      "loss": 2.3661,
      "step": 198
    },
    {
      "epoch": 0.14606844664648133,
      "grad_norm": 2.170541286468506,
      "learning_rate": 3.9800000000000005e-05,
      "loss": 2.1264,
      "step": 199
    },
    {
      "epoch": 0.14680245894118726,
      "grad_norm": 2.1549222469329834,
      "learning_rate": 4e-05,
      "loss": 2.1623,
      "step": 200
    },
    {
      "epoch": 0.1475364712358932,
      "grad_norm": 1.85322904586792,
      "learning_rate": 4.02e-05,
      "loss": 2.2447,
      "step": 201
    },
    {
      "epoch": 0.14827048353059913,
      "grad_norm": 2.2842814922332764,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 2.2209,
      "step": 202
    },
    {
      "epoch": 0.14900449582530506,
      "grad_norm": 2.8345131874084473,
      "learning_rate": 4.0600000000000004e-05,
      "loss": 2.2348,
      "step": 203
    },
    {
      "epoch": 0.14973850812001102,
      "grad_norm": 2.32028865814209,
      "learning_rate": 4.08e-05,
      "loss": 2.2555,
      "step": 204
    },
    {
      "epoch": 0.15047252041471695,
      "grad_norm": 2.1111395359039307,
      "learning_rate": 4.1e-05,
      "loss": 2.0797,
      "step": 205
    },
    {
      "epoch": 0.1512065327094229,
      "grad_norm": 2.069866895675659,
      "learning_rate": 4.12e-05,
      "loss": 2.3349,
      "step": 206
    },
    {
      "epoch": 0.15194054500412882,
      "grad_norm": 2.013530969619751,
      "learning_rate": 4.14e-05,
      "loss": 2.208,
      "step": 207
    },
    {
      "epoch": 0.15267455729883475,
      "grad_norm": 2.2132787704467773,
      "learning_rate": 4.16e-05,
      "loss": 2.2438,
      "step": 208
    },
    {
      "epoch": 0.15340856959354068,
      "grad_norm": 2.462411642074585,
      "learning_rate": 4.18e-05,
      "loss": 2.3118,
      "step": 209
    },
    {
      "epoch": 0.15414258188824662,
      "grad_norm": 1.9923855066299438,
      "learning_rate": 4.2e-05,
      "loss": 2.0991,
      "step": 210
    },
    {
      "epoch": 0.15487659418295258,
      "grad_norm": 1.8137468099594116,
      "learning_rate": 4.22e-05,
      "loss": 2.3622,
      "step": 211
    },
    {
      "epoch": 0.1556106064776585,
      "grad_norm": 2.453098773956299,
      "learning_rate": 4.24e-05,
      "loss": 2.2337,
      "step": 212
    },
    {
      "epoch": 0.15634461877236444,
      "grad_norm": 2.448686361312866,
      "learning_rate": 4.26e-05,
      "loss": 2.1563,
      "step": 213
    },
    {
      "epoch": 0.15707863106707037,
      "grad_norm": 2.0328216552734375,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 2.411,
      "step": 214
    },
    {
      "epoch": 0.1578126433617763,
      "grad_norm": 2.058260440826416,
      "learning_rate": 4.3e-05,
      "loss": 2.2651,
      "step": 215
    },
    {
      "epoch": 0.15854665565648224,
      "grad_norm": 2.2139711380004883,
      "learning_rate": 4.32e-05,
      "loss": 2.2969,
      "step": 216
    },
    {
      "epoch": 0.15928066795118817,
      "grad_norm": 2.288782835006714,
      "learning_rate": 4.3400000000000005e-05,
      "loss": 2.1691,
      "step": 217
    },
    {
      "epoch": 0.16001468024589413,
      "grad_norm": 2.287349224090576,
      "learning_rate": 4.36e-05,
      "loss": 2.1755,
      "step": 218
    },
    {
      "epoch": 0.16074869254060006,
      "grad_norm": 2.268669366836548,
      "learning_rate": 4.38e-05,
      "loss": 2.313,
      "step": 219
    },
    {
      "epoch": 0.161482704835306,
      "grad_norm": 2.763073205947876,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 2.4875,
      "step": 220
    },
    {
      "epoch": 0.16221671713001193,
      "grad_norm": 1.9758903980255127,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 2.187,
      "step": 221
    },
    {
      "epoch": 0.16295072942471786,
      "grad_norm": 2.194411039352417,
      "learning_rate": 4.44e-05,
      "loss": 2.2834,
      "step": 222
    },
    {
      "epoch": 0.1636847417194238,
      "grad_norm": 2.0634803771972656,
      "learning_rate": 4.46e-05,
      "loss": 2.3114,
      "step": 223
    },
    {
      "epoch": 0.16441875401412973,
      "grad_norm": 2.1120221614837646,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 2.1203,
      "step": 224
    },
    {
      "epoch": 0.16515276630883569,
      "grad_norm": 2.4735374450683594,
      "learning_rate": 4.5e-05,
      "loss": 2.2467,
      "step": 225
    },
    {
      "epoch": 0.16588677860354162,
      "grad_norm": 2.1523807048797607,
      "learning_rate": 4.52e-05,
      "loss": 2.0976,
      "step": 226
    },
    {
      "epoch": 0.16662079089824755,
      "grad_norm": 2.293163776397705,
      "learning_rate": 4.5400000000000006e-05,
      "loss": 2.1663,
      "step": 227
    },
    {
      "epoch": 0.16735480319295348,
      "grad_norm": 2.9858922958374023,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 2.075,
      "step": 228
    },
    {
      "epoch": 0.16808881548765942,
      "grad_norm": 1.9942705631256104,
      "learning_rate": 4.58e-05,
      "loss": 2.328,
      "step": 229
    },
    {
      "epoch": 0.16882282778236535,
      "grad_norm": 2.3202097415924072,
      "learning_rate": 4.600000000000001e-05,
      "loss": 2.1546,
      "step": 230
    },
    {
      "epoch": 0.16955684007707128,
      "grad_norm": 1.9657279253005981,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 2.2761,
      "step": 231
    },
    {
      "epoch": 0.17029085237177724,
      "grad_norm": 2.383000612258911,
      "learning_rate": 4.64e-05,
      "loss": 2.1899,
      "step": 232
    },
    {
      "epoch": 0.17102486466648317,
      "grad_norm": 1.8771861791610718,
      "learning_rate": 4.660000000000001e-05,
      "loss": 2.4404,
      "step": 233
    },
    {
      "epoch": 0.1717588769611891,
      "grad_norm": 2.659254550933838,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 2.0787,
      "step": 234
    },
    {
      "epoch": 0.17249288925589504,
      "grad_norm": 2.1125686168670654,
      "learning_rate": 4.7e-05,
      "loss": 2.1155,
      "step": 235
    },
    {
      "epoch": 0.17322690155060097,
      "grad_norm": 3.2469890117645264,
      "learning_rate": 4.72e-05,
      "loss": 2.2692,
      "step": 236
    },
    {
      "epoch": 0.1739609138453069,
      "grad_norm": 2.0443592071533203,
      "learning_rate": 4.74e-05,
      "loss": 2.1114,
      "step": 237
    },
    {
      "epoch": 0.17469492614001284,
      "grad_norm": 2.412787914276123,
      "learning_rate": 4.76e-05,
      "loss": 2.1367,
      "step": 238
    },
    {
      "epoch": 0.1754289384347188,
      "grad_norm": 1.8797932863235474,
      "learning_rate": 4.78e-05,
      "loss": 2.4456,
      "step": 239
    },
    {
      "epoch": 0.17616295072942473,
      "grad_norm": 2.337599992752075,
      "learning_rate": 4.8e-05,
      "loss": 2.6135,
      "step": 240
    },
    {
      "epoch": 0.17689696302413066,
      "grad_norm": 2.0837597846984863,
      "learning_rate": 4.82e-05,
      "loss": 2.1596,
      "step": 241
    },
    {
      "epoch": 0.1776309753188366,
      "grad_norm": 2.081875801086426,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 2.2467,
      "step": 242
    },
    {
      "epoch": 0.17836498761354252,
      "grad_norm": 2.27854585647583,
      "learning_rate": 4.86e-05,
      "loss": 2.2523,
      "step": 243
    },
    {
      "epoch": 0.17909899990824846,
      "grad_norm": 2.292546272277832,
      "learning_rate": 4.88e-05,
      "loss": 2.1656,
      "step": 244
    },
    {
      "epoch": 0.1798330122029544,
      "grad_norm": 2.3714730739593506,
      "learning_rate": 4.9e-05,
      "loss": 2.365,
      "step": 245
    },
    {
      "epoch": 0.18056702449766032,
      "grad_norm": 2.1320364475250244,
      "learning_rate": 4.92e-05,
      "loss": 2.0815,
      "step": 246
    },
    {
      "epoch": 0.18130103679236628,
      "grad_norm": 2.005913257598877,
      "learning_rate": 4.94e-05,
      "loss": 2.0435,
      "step": 247
    },
    {
      "epoch": 0.18203504908707221,
      "grad_norm": 2.1006031036376953,
      "learning_rate": 4.96e-05,
      "loss": 2.2914,
      "step": 248
    },
    {
      "epoch": 0.18276906138177815,
      "grad_norm": 2.322671413421631,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 2.1286,
      "step": 249
    },
    {
      "epoch": 0.18350307367648408,
      "grad_norm": 2.2516791820526123,
      "learning_rate": 5e-05,
      "loss": 2.5147,
      "step": 250
    },
    {
      "epoch": 0.18423708597119,
      "grad_norm": 1.922756314277649,
      "learning_rate": 5.02e-05,
      "loss": 2.2286,
      "step": 251
    },
    {
      "epoch": 0.18497109826589594,
      "grad_norm": 1.9841333627700806,
      "learning_rate": 5.0400000000000005e-05,
      "loss": 2.2324,
      "step": 252
    },
    {
      "epoch": 0.18570511056060188,
      "grad_norm": 2.320833206176758,
      "learning_rate": 5.0600000000000003e-05,
      "loss": 2.0853,
      "step": 253
    },
    {
      "epoch": 0.18643912285530784,
      "grad_norm": 2.1627213954925537,
      "learning_rate": 5.08e-05,
      "loss": 2.2196,
      "step": 254
    },
    {
      "epoch": 0.18717313515001377,
      "grad_norm": 1.951710820198059,
      "learning_rate": 5.1000000000000006e-05,
      "loss": 2.1595,
      "step": 255
    },
    {
      "epoch": 0.1879071474447197,
      "grad_norm": 2.670639753341675,
      "learning_rate": 5.1200000000000004e-05,
      "loss": 2.1382,
      "step": 256
    }
  ],
  "logging_steps": 1,
  "max_steps": 256,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.3250122355769344e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
