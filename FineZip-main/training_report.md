# FineZip 训练报告

## 1. 基本信息
- **模型**: Llama-3.1-8B
- **训练时长**: 41分03秒 (2463.98秒)
- **训练速度**: 3.325 samples/second
- **总步数**: 256步
- **平均步时**: 9.62秒/步

## 2. 硬件与环境
- **GPU**: NVIDIA GeForce RTX 3060 Ti (8GB)
- **PyTorch版本**: 2.0.1+cu117
- **CUDA版本**: 11.7
- **主要依赖**:
  - transformers >= 4.31.0
  - peft >= 0.4.0
  - bitsandbytes >= 0.41.1

## 3. 模型配置
### 量化设置
- **量化精度**: 4-bit (四分之一精度)
- **计算数据类型**: float16 (半精度浮点)
- **量化类型**: nf4 (嵌套浮点4位)
- **双重量化**: 启用 (进一步降低内存占用)

### LoRA配置
- **秩 (rank)**: 8 (低秩分解的维度)
- **缩放因子**: 32 (LoRA更新的缩放比例)
- **Dropout率**: 0.05 (防止过拟合的随机丢弃率)
- **偏置处理**: none (不更新偏置参数)
- **任务类型**: CAUSAL_LM (因果语言建模)

这些配置的主要目的是：
1. 通过4-bit量化大幅降低模型内存占用
2. 使用LoRA技术实现高效的参数微调
3. 在保持模型性能的同时最小化显存需求

## 4. 训练参数
- **批次大小**: 4
- **梯度累积步数**: 8
- **学习率**: 1e-4
- **块大小**: 64 tokens
- **预热步数**: 500
- **权重衰减**: 0.01
- **梯度裁剪**: 1.0

## 5. 训练效果
### Loss变化
- **初始**: 2.64 (正确率 ~7.1%)
- **中期**: 2.40 (正确率 ~9.1%)
- **最终平均**: 2.40 (正确率 ~9.1%)
- **最佳**: 2.04 (正确率 ~13.0%)

### 梯度统计
- **初始阶段**: 0.78-0.91
- **中期阶段**: 1.5-2.0
- **后期阶段**: 2.0-3.0

### 学习率变化
- **起始**: 2e-7
- **结束**: 5.12e-5
- **策略**: 线性warmup

## 6. 存储信息
### 目录结构
FineZip-main/
├── finetuned_models/
│ └── 10mb_qlora/
│ └── 4bit/
│ └── meta-llama-Llama-3.1-8B-enwik10mb_256_r8/
├── output/
│ └── meta-llama-Llama-3.1-8B/
│ ├── checkpoint-50/
│ ├── checkpoint-100/
│ └── logs/


## 7. 训练特点
1. 使用4-bit量化显著降低显存占用
2. 采用LoRA进行参数高效微调
3. 启用梯度检查点节省显存
4. 每50步保存一次检查点
5. 训练过程稳定，无梯度爆炸现象

## 8. 结论
1. 训练总体成功完成，loss呈下降趋势
2. 预测准确率从初始的7.1%提升至平均9.1%
3. 最佳时刻达到13.0%的准确率
4. 训练过程稳定，未出现异常波动
5. 4-bit量化和LoRA的组合有效降低了显存需求
